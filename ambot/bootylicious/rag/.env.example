# =============================================================================
# Ambot RAG System - Environment Configuration
# =============================================================================
# Copy this file to .env and customize as needed
# cp .env.example .env
# =============================================================================

# -----------------------------------------------------------------------------
# Network
# -----------------------------------------------------------------------------
NETWORK_NAME=ambot-rag
API_PORT=8000

# -----------------------------------------------------------------------------
# PostgreSQL
# -----------------------------------------------------------------------------
POSTGRES_DB=ambot_rag
POSTGRES_USER=ambot
POSTGRES_PASSWORD=ambot_secure_pass

# -----------------------------------------------------------------------------
# Embedding Model
# -----------------------------------------------------------------------------
# Using sentence-transformers (runs locally in API container)
EMBEDDING_MODEL=all-MiniLM-L6-v2
EMBEDDING_DIMENSION=384
EMBEDDING_BACKEND=sentence-transformers

# -----------------------------------------------------------------------------
# Chunking
# -----------------------------------------------------------------------------
CHUNK_SIZE=512
CHUNK_OVERLAP=50

# -----------------------------------------------------------------------------
# LLM Backend
# -----------------------------------------------------------------------------
# Options: "ollama" or "huggingface"
# - ollama: Connects to external Ollama server
# - huggingface: Runs model locally with transformers
LLM_BACKEND=ollama

# -----------------------------------------------------------------------------
# Ollama Configuration (if LLM_BACKEND=ollama)
# -----------------------------------------------------------------------------
# NOTE: host.docker.internal doesn't work on Jetson/Linux.
# Use the Docker bridge gateway IP instead (check with: docker network inspect ambot-rag | grep Gateway)
OLLAMA_BASE_URL=http://172.18.0.1:11434
# Recommended: llama3.2:3b (2 GB, much better context grounding than tinyllama)
# tinyllama hallucinates; llama3.2:3b answers from retrieved context accurately
LLM_MODEL=llama3.2:3b
LLM_TEMPERATURE=0.3
LLM_TIMEOUT=300
# Context window: 4096 is Ollama default for <24 GiB VRAM (Jetson has 7.4 GiB)
# Increase to 8192 if RAG chunks are large, but watch memory usage
LLM_NUM_CTX=4096
# Max response tokens: default 128 truncates RAG answers with citations
# 512 is enough for concise answers; increase to 1024 for detailed responses
LLM_NUM_PREDICT=512

# -----------------------------------------------------------------------------
# HuggingFace Configuration (if LLM_BACKEND=huggingface)
# -----------------------------------------------------------------------------
HF_MODEL=TinyLlama/TinyLlama-1.1B-Chat-v1.0
HF_DEVICE=cuda
HF_TORCH_DTYPE=float16
HF_CACHE=~/.cache/huggingface

# -----------------------------------------------------------------------------
# RAG Retrieval
# -----------------------------------------------------------------------------
RAG_TOP_K=3
RAG_MIN_SIMILARITY=0.75

# -----------------------------------------------------------------------------
# Project
# -----------------------------------------------------------------------------
PROJECT_NAME=ambot-eecs

# -----------------------------------------------------------------------------
# Knowledge Base
# -----------------------------------------------------------------------------
# Directory containing documents to ingest
KNOWLEDGE_DIR=./knowledge
